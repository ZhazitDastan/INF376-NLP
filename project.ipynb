{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a93d1512-dfd0-413b-b9a0-29306cf38534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "dataset = load_dataset(\"issai/kazqad\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples['question']]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples['context'],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        stride=128,\n",
    "    )\n",
    "    \n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    \n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        input_ids = inputs[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        sample_index = sample_mapping[i]\n",
    "        answer = answers[sample_index]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        \n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "        \n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "        \n",
    "        if not (offset[token_start_index][0] <= start_char and offset[token_end_index][1] >= end_char):\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            while token_start_index < len(offset) and offset[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            start_positions.append(token_start_index - 1)\n",
    "            \n",
    "            while offset[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            end_positions.append(token_end_index + 1)\n",
    "    \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "300d1207-2b38-411f-86d7-3b20f30177d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question:  Майкл Джордан НБА-да қашан баскетбол ойнады?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers: 1980-1990\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question:  exite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer not found in the dataset.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program terminated.\n"
     ]
    }
   ],
   "source": [
    "def extract_qa_pairs(dataset):\n",
    "    qa_pairs = []\n",
    "    for split in dataset.keys():\n",
    "        for data in dataset[split]:\n",
    "            question = data['question']\n",
    "            answers = data['answers']['text'] if 'text' in data['answers'] and data['answers']['text'] else [\n",
    "                \"Answer not found\"]\n",
    "            qa_pairs.append({'question': question, 'answers': answers})\n",
    "    return qa_pairs\n",
    "\n",
    "qa_pairs = extract_qa_pairs(dataset)\n",
    "\n",
    "def find_answers_for_question(question, qa_pairs):\n",
    "    for pair in qa_pairs:\n",
    "        if pair['question'] == question:\n",
    "            return pair['answers']\n",
    "    return [\"Answer not found\"]\n",
    "\n",
    "while True:\n",
    "    input_question = input(\"Enter your question: \")\n",
    "\n",
    "    if input_question.lower() == 'exit':\n",
    "        print(\"Program terminated.\")\n",
    "        break\n",
    "\n",
    "    answers = find_answers_for_question(input_question, qa_pairs)\n",
    "\n",
    "    if answers != [\"Answer not found\"]:\n",
    "        print(\"Answers:\", \", \".join(answers))\n",
    "        print()\n",
    "    else:\n",
    "        print(\"Answer not found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a0a78-f4d4-47aa-8049-63f717b180da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
